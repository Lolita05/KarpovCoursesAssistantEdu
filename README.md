# LLM РЕКОМЕНДАТОР (RAG)

**Трек**: DS  
**Сложность**: Medium  
**Темы**: RAG, Python, Streamlit, Embeddings  

## История

Вы — аналитик в новом подразделении KarpovCourses под названием EduTech Lab. Команда всерьёз занялась внедрением современных AI-технологий, чтобы помогать потенциальным студентам выбирать самые подходящие для них курсы. Руководство поставило задачу:

> "Сделать сервис, который на основе лендингов наших курсов сможет отвечать на вопросы пользователя и давать рекомендации. Что-то вроде чат-бота, который действительно понимает наши продукты и может подсказывать курс, максимально отвечающий запросам!"

Чтобы реализовать такую функциональность, вы решаете использовать подход Retrieval-Augmented Generation (RAG). Он позволит «подкрепить» ответы модели актуальными данными с лендингов KarpovCourses.

### Что же такое RAG
**RAG (Retrieval-Augmented Generation)** — это подход, при котором LLM дополняется актуальными данными из внешнего источника. По сути, вместо того чтобы отвечать «из головы» (из предобученной модели), мы извлекаем нужные документы из базы знаний (retrieval), передаём их как контекст, и модель генерирует (generation) ответ, учитывая этот контекст.

Как это работает (упрощённо):

1. **Пользователь задаёт вопрос**: например, «Какие курсы есть на Karpov.Courses по анализу данных?».
2. Система ищет релевантные документы ( **Retrieval** ) в своей базе знаний (например, тексты лендингов курсов).
3. Система «склеивает» эти документы с вопросом в единый prompt и передаёт его в LLM.
4. Модель формирует ответ ( **Generation** ), используя как внутренние знания, так и предоставленный контекст (кусочки найденных документов).

Ключевые преимущества:
- **Точность**: берем актуальные данные, избегаем «галлюцинаций»;
- **Гибкость**: можно добавлять новую информацию «на лету»;
- **Простота**: не надо дообучать модель, достаточно снабжать её правильным контекстом.

## Общий план

По шагам создадим RAG-сервис, который:
1. Парсит тексты с лендинга курсов;
2. Делит текст на чанки и обрабатывает (при необходимости удаляет мусор, лишние пробелы и т.п.);
3. Генерирует эмбеддинги для каждого чанка и сохраняет их в «базу знаний»;
4. Обрабатывает пользовательский запрос: находит самые близкие чанки и формирует ответ.
5. Обернёт всё это в простое веб-приложение (Streamlit).

Каждый из этих шагов — это отдельное задание в LMS. После выполнения шага вы отправляете код или решение на проверку. При желании вы можете собрать из всех шагов свое готовое приложение, а затем самостоятельно его задеплоить.

### Подсказки и советы
- Запускайте тесты как можно раньше, чтобы проверить, корректно ли вы решили задание.
- Пишите простые решения; главное — чтобы всё работало и было понятно.
- Соблюдайте **PEP8** и базовые правила оформления кода: это повысит читабельность и облегчает проверку.
- Используйте ChatGPT как «советника», если где-то застряли. 

Удачи! Когда все шаги будут выполнены, вы сможете похвастаться своим мини-RAG-сервисом для KarpovCourses.

## А теперь сами Задания (по шагам)

## Задание 1. Парсинг

### Описание задачи

Чтобы LLM могла «знать» о наших курсах, нужно **спарсить** данные с лендинга Karpov.Courses — [https://karpov.courses/](https://karpov.courses/)  
Цель — получить строки, которые в дальнейшем будем делить на чанки.

### Что нужно сделать

1.	Создать модуль (файл) `parse_data.py` с функцией, которая:
```python
def parse_karpov_landing(url: str) -> str:
	# your logic
	return text
```
- Принимает на вход url.
- Возвращает спарсенный текст (в 1 строку).
2. Убедиться, что при корректном URL результат не пустой (допустимо > 100 символов).
3. При желании — обработать возможные ошибки (например, недоступный сайт).

#### Полезные ссылки
- [Requests docs](https://requests.readthedocs.io/en/latest/)
- [BeautifulSoup docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium docs](https://www.selenium.dev/documentation/)

### Формат сдачи
- Загрузите файл `parse_data.py` с готовым решением.

Распределение баллов:  
**60%** — код отрабатывает корректно, текст успешно парсится (без крашей)  
**20%** — скорость обработки удовлетворяет требованиям  
**20%** — качество кода (pylint score; соблюдение PEP8)  
Если код сваливается с ошибкой, за все остальные проверки зачисляются 0 баллов.  

#### Подсказка #1
Для упрощения можно обойтись связкой requests + BeautifulSoup, так как лендинг не динамический.
```python
import requests
from bs4 import BeautifulSoup

# Инициализация response и beautifulsoup
```
#### Подсказка #2
Используйте сепаратор пробел `(separator=" ")`

#### Подсказка #3
Можно усложнить код и добавить связку undetected-chromedriver + Selenium. Оно учтет вариант, если сайт динамический (в т.ч. React/JS-фреймворки) и важный контент подгружается после рендеринга

```python
import time
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
```

## Задание 2. Чанкование и предобработка

### Описание задачи
Полученный текст может быть слишком большим. Длинные простыни неэффективно использовать для поиска релевантности. Нужно разделить текст на чанки (кусочки), например, по 500-800 символов.

### Что нужно сделать
1.	Создайте `chunk_text.py` с функцией, которая:
```python
def chunk_text(text: str, chunk_size=800) -> list:
	# your logic
	return chunks
```
- Принимает строку
- Разделяет на чанки, размер каждого (N символов или слов) задаёте сами (например, 800 символов)
- Возвращает список чанков (строк)
2.	(Опционально) можно убрать HTML-теги, лишние пробелы, «мусор» вроде скриптов

#### Полезные ссылки
- [Regex docs (если нужно что-то вырезать)](https://docs.python.org/3/howto/regex.html)
- [Regular expressions cookbook](https://docs.python.org/3/library/re.html)

### Формат сдачи
- Загрузите файл `chunk_text.py` с готовым решением.

Распределение баллов:  
**60%** — код отрабатывает корректно, при подаче длинного текста вы получаете нужное количество чанков, и каждый чанк не превышает заданный лимит.  
**20%** — скорость обработки удовлетворяет требованиям  
**20%** — качество кода (pylint score; соблюдение PEP8)  
Если код сваливается с ошибкой, за все остальные проверки зачисляются 0 баллов.

#### Подсказка #1
Если хотите разбивать по словам, используйте `text.split()` и считайте, сколько слов помещается в `chunk_size`.
Если разбиваете по символам, учитывайте пробелы `(len(token) + 1 при склейке)`.

#### Подсказка #2
Если после чанкования возвращается пустой список, проверьте, не обрезали ли вы текст «неправильной» логикой.

# Задание 3. Генерация эмбеддингов и создание индекса

### Описание задачи
Чтобы быстро находить «похожие» чанки, используем эмбеддинги — векторные представления текста. Любая современная LLM или эмбеддинговая модель может превратить текст в вектор.

### Что нужно сделать
1. Создать create_embeddings.py:
```python
def create_embeddings(chunks: list, model_name: str = "text-embedding-ada-002") -> list:
	"""
    Принимает список чанков (строк).
    Возвращает список (chunk_text, embedding_vector).
    """
	# your logic
	return results
```
- Функция, принимающая список чанков (строк).
- Возвращающая список кортежей (chunk_text, embedding_vector).
2.	(Опционально) Сохранить полученные эмбеддинги в локальный файл (pickle/JSON), чтобы не перегенерировать их на каждом запуске.

#### Полезные ссылки
- [OpenAI Embeddings docs](https://platform.openai.com/docs/guides/embeddings)
- [SentenceTransformers docs](https://www.sbert.net/)
- [FAISS docs](https://github.com/facebookresearch/faiss)

### Формат сдачи
- Загрузите файл `create_embeddings.py` с готовым решением.

Распределение баллов:  
**60%** — код отрабатывает корректно, возвращаемый вектор не пустой, и длина списка эмбеддингов совпадает с количеством чанков.  
**20%** — скорость обработки удовлетворяет требованиям  
**20%** — качество кода (pylint score; соблюдение PEP8)  
Если код сваливается с ошибкой, за все остальные проверки зачисляются 0 баллов.

#### Подсказка #1
Самый простой способ использовать OpenAI Embeddings, но он требует платные ключи OpenAI
Если вам подходит такой способ, не забудьте создать файл `.env` и положить в него ключ формата `OPENAI_API_KEY="YOUR_OPENAI_API_KEY"`

#### Подсказка #2
Если не хотите/не можете использовать платные ключи OpenAI, то можно воспользоваться локальной моделью (например, sentence-transformers/all-MiniLM-L6-v2)

#### Подсказка #3
(Опционально) можете реализовать простой класс «MemoryIndex», который хранит эмбеддинги в Python-списке, либо используйте FAISS/Pinecone

# Задание 4. Обработка пользовательского запроса
### Описание задачи
Теперь, когда у нас есть эмбеддинги каждого чанка, мы хотим обрабатывать вопрос пользователя:
1. Считать эмбеддинг запроса.
2. Сравнить его с эмбеддингами каждого чанка (косинусное сходство).
3. Отсортировать по убыванию сходства.
4. Вернуть top-k релевантных чанков.

### Что нужно сделать

1. Создать find_top_k.py, где:
```python
def find_top_k(user_query: str, embeddings_list: list, k=3, model_name: str = "text-embedding-ada-002"):
    """
    user_query: вопрос пользователя
    embeddings_list: [(chunk_text, embedding_vector), ...]
	:param model_name: Модель эмбеддингов для генерации вектора запроса.
    :return: Список (chunk_text, score) размером k
    """
    # Генерируем эмбеддинг запроса
    # Считаем косинусное сходство
    # Сортируем
    # Возвращаем top-k
    pass
```
- На вход: `user_query`, список (`chunk_text`, `embedding`).
- Генерирует эмбеддинг запроса (тем же методом, что и чанки).
- Находит самые близкие чанки.
- Возвращает список (`chunk_text`, `similarity_score`) или просто тексты.

2. Создать функцию `generate_answer()`, которая вставляет найденные чанки в prompt и запрашивает LLM:
```python
def generate_answer(context_chunks: list, user_query: str, model_name: str = "gpt-3.5-turbo") -> str:
    """
    Формирует ответ на основе контекста (чанков) и вопроса пользователя

    :param context_chunks: Список чанков, которые послужат контекстом.
    :param user_query: Вопрос пользователя (строка).
    :param model_name: Название модели.
    :return: Ответ от модели (строка).
    """
	# your logic
	return answer
```

#### Полезные ссылки
- [NumPy docs](https://numpy.org/doc/stable/)
- [OpenAI ChatCompletion docs](https://platform.openai.com/docs/guides/chat)

### Формат сдачи
- Загрузите файл `find_top_k.py` с готовым решением.

Распределение баллов:  
**60%** — код отрабатывает корректно, реально находит «похожие» чанки (тест проверит).  
**20%** — скорость обработки удовлетворяет требованиям  
**20%** — качество кода (pylint score; соблюдение PEP8)  
Если код сваливается с ошибкой, за все остальные проверки зачисляются 0 баллов.

#### Подсказка #1
Косинусное сходство можно вычислять через numpy или любые другие библиотеки.
```python
def cosine_similarity(vec_a, vec_b):
    return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))
```

#### Подсказка #2
Если используете OpenAI, для эмбеддинга запроса делайте `openai.Embedding.create(input=user_query, model=...)`.

#### Подсказка #3
`generate_answer()` может выглядеть так:
```python
context = "\n".join([chunk_text for chunk_text, _ in top_chunks])
prompt = f"Context: {context}\nUser: {user_query}\nAnswer:"
```
После чего используйте `openai.ChatCompletion.create(...)` или аналог.

# Задание 5. Реализация веб-сервиса через Streamlit

### Описание задачи
Это **финальное** задание (не проверяется автоматически). Нужно сделать простое **веб-приложение** на базе Streamlit, чтобы пользователь мог:
1.	Ввести вопрос.
2.	Получить ответ, основанный на самых релевантных чанках.

### Что нужно сделать

1. Подготовить свой проект под нужный формат
Например:
```python
.
├── streamlit_app.py   # Streamlit-приложение
├── requirements.txt
└── src
├── __init__.py
├── parse_data.py      # Парсинг
├── chunk_data.py      # Функции для чанкования текста
├── build_embedding.py # Генерация эмбеддингов
└── query_service.py   # Поиск релевантных чанков и вызов
```

2. Создать `streamlit_app.py`:
Пример:
```python
import streamlit as st
import openai
from src.parse_data import answer_query  # пример
...

def main():
    st.title("KarpovCourses RAG Demo")
    user_query = st.text_input("Ваш вопрос?")
    if st.button("Задать вопрос"):
        # Предположим, что embeddings_list уже загружен или сгенерирован
        top_k_chunks = answer_query(user_query, st.session_state["embeddings_list"], k=3)
        # (Опционально) Запрос к модели:
        answer = "Your final answer here..."
        st.write(answer)

if __name__ == "__main__":
    main()
```
3.	Внутри — код, который:
- Имеет поле ввода (`st.text_input`).
- Кнопку «Задать вопрос».
- При нажатии, находит релевантные чанки, формирует prompt и выдаёт ответ.


#### Подсказка #1
Храните промежуточные результаты (например, тексты, эмбеддинги) в `st.session_state`, чтобы не пересоздавать всё при каждом нажатии кнопки.

#### Подсказка #2
При деплое на Streamlit Cloud убедитесь, что у вас прописаны все зависимости в `requirements.txt`.

#### Подсказка #3
Если OpenAI key хранится в `.env`, нужно либо загрузить его в настройки Streamlit Cloud, либо прописать в `secrets.toml`.

